<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="assets/css/main.css">
    <title>DexCap | Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</title>


</head>
<body>

<!--<div class="full-page-image">-->
<!--    <video id="bg-video" autoplay loop muted playsinline>-->
<!--        <source src="assets/videos/full_screen.mp4" type="video/mp4">-->
<!--    </video>-->
<!--    <div class="overlay"></div>-->
<!--    <div class="content" style="padding: 0 20px">-->
<!--        <h1>T<span style="font-variant-caps:all-small-caps;">RANSIC</span>: Sim-to-Real Policy Transfer by Learning from-->
<!--            Online Correction</h1>-->
<!--        <p>An RL sim-to-real policy trained with T<span style="font-variant-caps:all-small-caps;">RANSIC</span>-->
<!--            successfully-->
<!--            completes a long-horizon and contact-rich task: assembling a table lamp from scratch.</p>-->
<!--    </div>-->
<!--</div>-->

<div id="title_slide">
    <div class="title_left">
        <h1>DexCap: Scalable and Portable Mocap Data<br>Collection System for Dexterous Manipulation</h1>
        <div class="author-container">
            <div class="author-name"><a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a></div>
            <div class="author-name"><a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a></div>
            <div class="author-name"><a href="https://tml.stanford.edu/people/weizhuo-ken-wang" target="_blank">Weizhuo Wang</a></div>
            <div class="author-name"><a href="https://ai.stanford.edu/~zharu" target="_blank">Ruohan Zhang</a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/c-karen-liu" target="_blank">C. Karen Liu</a></div>
        </div>
        <div class="affiliation">
            <p><img src="assets/logos/SUSig-red.png" style="height: 50px"></p>
        </div>
        <div class="button-container">
            <a href="" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <a href="" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="" target="_blank" class="button"><i class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>
            <a href="" target="_blank" class="button"><i class="fa-light fa-robot-astromech"></i>&emsp14;Hardware</a>
        </div>

        <br>
        <div class="slideshow-container">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata" width="100%">
                    <source src="assets/web_video_first.mov" type="video/mp4">
                </video>
<!--                <div class="caption" style="margin-top: -2.0vw">-->
<!--                    <p> Our controller is a causal Transformer trained by autoregressive prediction of future actions from-->
<!--                        the observation-action history.</p>-->
<!--                </div>-->
            </div>
        </div>

<!--        <div class="slideshow-container">-->
<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/lamp1.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Assemble a Table Lamp from Scratch</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/stabilize.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Stabilize the Square Tabletop by Pushing It to the Right Corner of the Wall</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/grasp_and_insert_bulb.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Grasp a Light Bulb and Insert It into the Lamp Base</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/insert.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Insert a Table Leg into the Assembly Holy of the Square Tabletop</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/grasp_bulb.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Reach and Grasp a Light Bulb</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/screw.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Screw a Table Leg into the Square Tabletop</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/grasp.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Reach and Grasp a Table Leg</div>-->
<!--            </div>-->

<!--            <div class="mySlides fade">-->
<!--                <video autoplay muted playsinline loop controls preload="metadata" width="100%">-->
<!--                    <source src="assets/videos/pull/screw_bulb.mp4" type="video/mp4">-->
<!--                </video>-->
<!--                <div class="text">Screw a Light Bulb into the Base</div>-->
<!--            </div>-->

<!--            &lt;!&ndash; Next and previous buttons &ndash;&gt;-->
<!--            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>-->
<!--            <a class="next" onclick="plusSlides(1)">&#10095;</a>-->
<!--        </div>-->
        <br>

<!--        &lt;!&ndash; The dots/circles &ndash;&gt;-->
<!--        <div style="text-align:center">-->
<!--            <span class="dot" onclick="currentSlide(1)"></span>-->
<!--            <span class="dot" onclick="currentSlide(2)"></span>-->
<!--            <span class="dot" onclick="currentSlide(3)"></span>-->
<!--            <span class="dot" onclick="currentSlide(4)"></span>-->
<!--            <span class="dot" onclick="currentSlide(5)"></span>-->
<!--            <span class="dot" onclick="currentSlide(6)"></span>-->
<!--            <span class="dot" onclick="currentSlide(7)"></span>-->
<!--            <span class="dot" onclick="currentSlide(8)"></span>-->
<!--        </div>-->

        <div id="abstract">
            <h1>Abstract</h1>
            <p>
                Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks.
                Despite this potential, substantial challenges persist. These include the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies.
                To tackle these issues, we introduce DexCap, a portable hand motion capture system. Alongside DexCap, we present DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data.
                DexCap offers precise, occlusion-resistant tracking of wrist and finger motions. It does this based on SLAM and electromagnetic fields, together with 3D observations of the environment.
                Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands.
                Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism. This occurs during policy rollouts to refine and further improve robot performance with minimal human effort.
                Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance. It also showcases the system's capability to effectively learn from in-the-wild mocap data.
                This paves the way for future data collection methods in the pursuit of human-level robot dexterity.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">

    <h1>DexCap: A Portable Hand Motion Capture System</h1>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/system_overview.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>Pulling a robot with a rope until it falls</p>-->
<!--            </div>-->
        </div>
    </div>
    <p>
        Overview of the DexCap system:
        <br><b>Front design:</b> A camera rack on the chest is equipped with an RGB-D LiDAR camera and three SLAM tracking cameras.
        <br><b>Back design:</b> A mini-PC and power bank in the backpack power the system for approximately 40 minutes of data collection.
        <br><b>Data collection process:</b> The tracking cameras, initially placed in the camera rack for calibration,
        are relocated to hand mounts during data collection to consistently track the palm positions.
        Finger motions are captured by motion capture gloves.
    </p>


    <h1>From Human to Robot</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/human_to_robot.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>Pulling a robot with a rope until it falls</p>-->
<!--            </div>-->
        </div>
    </div>
    <p>
        <b>Observation retargeting:</b> To simplify the process of switching the camera system between the human and robot,
        a quick-release buckle has been integrated into the back of the camera rack, allowing for swift camera swaps
        – in less than 20 seconds. In this way, the robot utilizes the same observation camera employed during human data collection.
    </p>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/fingertip_ik.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>Pulling a robot with a rope until it falls</p>-->
<!--            </div>-->
        </div>
    </div>
    <p>
        <b>Action retargeting:</b> To transfer human finger motion to the LEAP robot hand, we use fingertip
        inverse kinematics (IK) to compute the 16-dimensional joint positions. Human finger motions are tracked
        using a pair of motion capture gloves, which measure the 3D positions of the fingers relative to the palm based on electromagnetic field (EMF).
    </p>

    <br>
    <br>
    <br>
    <hr class="rounded">

    <h1>Method: Data Retargeting and Imitation Learning</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/method2.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>Pulling a robot with a rope until it falls</p>-->
<!--            </div>-->
        </div>
    </div>
    <p>
        We first retarget the DexCap data to the robot embodiment by constructing 3D point clouds from RGB-D observations and transforming it into robot operation space.
        Meanwhile, the hand motion capture data is retargeted to the dexterous hand and robot arm with fingertip IK.
        Based on the data, a Diffusion Policy is learned to take the point cloud as input and outputs a sequence of future goal positions as the robot actions.
    </p>

    <h1>Results</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_ball.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts. Policy learned with 30-minute human mocap data without any teleoperation.</p>
            </div>
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->

    <h1>Bimanual Manipulation Task</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_bimaual.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>0:00-0:09 Collecting bimanual human mocap data <br>0:10-1:47 Fully autonomous policy rollouts (learned with 30-minute human mocap data without any teleoperation)</p>
            </div>
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->

    <br>
    <hr class="rounded">

    <h1>In-the-wild Data Collection with DexCap</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/system_in_wild.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>0:00-0:09 collecting bimanual human mocap data; 0:10-1:47 fully autonomous policy rollouts (learned with 30-minute human mocap data without any teleoperation)</p>-->
<!--            </div>-->
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->

    <h1>Policy learned with In-the-wild DexCap Data</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_trainedobj.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Trained objects:</b> Fully autonomous policy rollouts in 1x speed.</p>
            </div>
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->

    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_unseenobj.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Unseen objects:</b>. Fully autonomous policy rollouts in 1x speed.</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_unseenobj2.mp4" type="video/mp4">
            </video>
        </div>
    </div>

<!--    <p>-->
<!--        xxx-->
<!--    </p>-->

    <br>
    <hr class="rounded">

    <h1>Human-in-the-loop correction with DexCap</h1>
<!--    <div class="image_container">-->
<!--        <img src="assets/HIL.png" alt="Description of Image">-->
<!--    </div>-->
    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/HIL_mode1.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>xxxx</p>-->
<!--            </div>-->
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/HIL_mode2.mp4" type="video/mp4">
            </video>
<!--            <div class="caption">-->
<!--                <p>xxxx</p>-->
<!--            </div>-->
        </div>
    </div>
    <p>
        DexCap supports two types of human-in-the-loop correction during the policy rollouts: <br>
        <b>(1). Residual correction</b> measures the 3D delta position changes of the human wrist and incorporates them as residual actions to the robot's wrist movements.
        This mode enables minimal movement but requiring more precise control.<br>
        <b>(2). Teleoperation</b> directly translates full human hand motions to the robot end-effector actions based on inverse kinematics.
        This mode enables the full control over the robot but requiring more effort.<br>
        Users can switch between the two modes by stepping on the foot pedal during the rollouts.
    </p>

    <div class="image_container">
        <img src="assets/HIL_method.png" alt="Description of Image">
    </div>
    <p>
        The corrections are stored in a new dataset and uniformly sampled with the original dataset for fine-tuning the robot policy
    </p>

    <h1>Results after finetuning - Tea preparing</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_tea.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts in 2x speed. Policy learned with 1-hour human mocap data and 30 human-in-the-loop corrections.</p>
            </div>
            <br>
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_tea_more.mp4" type="video/mp4">
            </video>
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->


    <h1>Results after finetuning - Scissor cutting</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_scissor.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts in 2x speed. Policy learned with 1-hour human mocap data and 30 human-in-the-loop corrections.</p>
            </div>
            <br>
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_scissor_more.mp4" type="video/mp4">
            </video>
        </div>
    </div>
<!--    <p>-->
<!--        xxx-->
<!--    </p>-->



<!--    <br>-->
<!--    <div class="allegrolower">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/forward.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Walking forward and backward</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/backward.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->

<!--    <h1>Walks on Different Surfaces</h1>-->

<!--    <p>-->
<!--        A humanoid robot should be able to walk over different terrains.-->
<!--        To assess the capabilities of our transformer-based controller in this regard, we conducted a series of-->
<!--        experiments on different terrains in the laboratory.-->
<!--        In each experiment, we command the robot to walk forward and vary the terrain type.-->
<!--        We first consider flat terrains with variation in friction (slippery plastic floor and carpet) and roughness-->
<!--        (wrapping bags and cables spread over the floor).-->
<!--        We find that our controller is able to handle these types of terrains, with the slippery plastic floor being the-->
<!--        most challenging.-->
<!--    </p>-->
<!--    <br>-->

<!--    <div class="allegroupper">-->
<!--        <video autoplay muted playsinline loop preload="metadata">-->
<!--            <source src="assets/digit/floor.mp4" type="video/mp4">-->
<!--        </video>-->
<!--        <video autoplay muted playsinline loop preload="metadata">-->
<!--            <source src="assets/digit/carpet.mp4" type="video/mp4">-->
<!--        </video>-->
<!--    </div>-->
<!--    <div class="allegrolower">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/foam.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Walking over flat terrains with different friction (top) and roughness (bottom)</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/cable.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->

<!--    <h1>Walks over Uneven Terrains</h1>-->
<!--    <p>-->
<!--        Next, we consider walking over slopes and small steps.-->
<!--        We note that our robot was not trained on steps in simulation.-->
<!--        We observe that our controller initially makes a mistake when ascending the step, but quickly corrects and-->
<!--        raises the leg higher and faster on the second attempt.-->
<!--    </p>-->

<!--    <br>-->
<!--    <div class="allegrolower">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/slope.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Walking over slopes (left) and steps (right)</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/stair.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->

<!--    <h1>Carries Payloads</h1>-->
<!--    <p>-->
<!--        We also evaluate the walking behavior when carrying loads of varying mass and shape.-->
<!--        Notice that our walking behaviors exhibit an emergent arm swing for balancing.-->
<!--        Placing loads on an arm interferes with this and requies the robot to adapt accordingly.-->
<!--    </p>-->

<!--    <div class="allegroupper">-->
<!--        <video autoplay muted playsinline loop preload="metadata">-->
<!--            <source src="assets/digit/empty_backpack.mp4" type="video/mp4">-->
<!--        </video>-->
<!--        <video autoplay muted playsinline loop preload="metadata">-->
<!--            <source src="assets/digit/trash_bag.mp4" type="video/mp4">-->
<!--        </video>-->
<!--    </div>-->
<!--    <div class="allegrolower">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/yorker_bag.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Carrying loads of varying mass and shape</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/TJ_bag.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->

<!--    <h1>Is Robust to Disturbances</h1>-->

<!--    <p>-->
<!--        Finally, we test the robustness of policies to sudden external forces.-->
<!--        We push the robot with a wooden stick or throw light cardboard boxes at it.-->
<!--    </p>-->
<!--    <div class="teleop">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/stick.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Applying suddent external forces</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop preload="metadata">-->
<!--                <source src="assets/digit/mujoco_box.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->

<!--    <h1> Failure Cases </h1>-->
<!--    <p>-->
<!--        While our policies show promising singal in the real world, they are certainly not without limitations.-->
<!--        For example, we perform an experiment to test the limits of the robustness of our controller.-->
<!--        We tie a cable to the back of the robot, command the robot to walk forward, and pull the robot backward.-->
<!--        We see that the robot is fairly robust but falls eventually when pulled very hard.-->
<!--    </p>-->

<!--    <div class="allegrofail">-->
<!--        <div class="video_container">-->
<!--            <video autoplay muted playsinline loop controls preload="metadata">-->
<!--                <source src="assets/digit/failure.mp4" type="video/mp4">-->
<!--            </video>-->
<!--            <div class="caption">-->
<!--                <p>Pulling a robot with a rope until it falls</p>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->

    <h1>BibTeX</h1>
    <p class="bibtex">
        xxx
<!--        @article{HumanoidTransformer2023,<br>-->
<!--        &nbsp;&nbsp;title={Learning Humanoid Locomotion with Transformers},<br>-->
<!--        &nbsp;&nbsp;author={Ilija Radosavovic and Tete Xiao and Bike Zhang and Trevor Darrell and Jitendra Malik and-->
<!--        Koushil Sreenath},<br>-->
<!--        &nbsp;&nbsp;year={2023},<br>-->
<!--        &nbsp;&nbsp;journal={arXiv:2303.03381}<br>-->
<!--        }-->
    </p>
</div>
</body>

<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
